{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(DATA_SOURCE: str,\n",
    "            model: str,\n",
    "            day: str,\n",
    "            train_test: str,\n",
    "            fold: int,\n",
    "            label_to_idx: dict,\n",
    "            ip_set: list = None):\n",
    "    \n",
    "    df = pd.read_csv(f\"data/2022/input/reps/out/k3/{DATA_SOURCE}/{train_test}/{model}_{day}_fold0{fold}.csv\")\n",
    "    df.sort_values(by=[\"src_ip\"], inplace=True)\n",
    "    # Taking the intersection.\n",
    "    df = df[df.src_ip.isin(ip_set)]\n",
    "    df_copy = df.drop(columns=[\"src_ip\"])\n",
    "    y = df_copy.label.values\n",
    "    X = df_copy.drop(columns=[\"label\"]).values\n",
    "    \n",
    "    return X, [ label_to_idx[i] for i in y]\n",
    "\n",
    "def load_train_test(sources: list,\n",
    "                    models: list,\n",
    "                    day: str,\n",
    "                    fold: int,\n",
    "                    label_to_idx: dict,\n",
    "                    inter_set_train: list,\n",
    "                    inter_set_test: list):\n",
    "    \n",
    "    X_train, X_test = [], []\n",
    "    for source, model in product(sources, models):\n",
    "        \n",
    "        X, y_train = get_X_y(source, model, day, \"train\", fold, label_to_idx, inter_set_train)\n",
    "        X_train.append(X)\n",
    "        \n",
    "        X, y_test = get_X_y(source, model, day, \"test\", fold, label_to_idx, inter_set_test)\n",
    "        X_test.append(X)\n",
    "    \n",
    "    X_train = np.hstack(X_train)\n",
    "    X_test = np.hstack(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "class DataHandler:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 sources: str,\n",
    "                 base_models: list,\n",
    "                 day: str,\n",
    "                 fold: int,\n",
    "                 label_to_idx: dict,\n",
    "                 inter_set_train: list,\n",
    "                 inter_set_test: list) -> None:\n",
    "    \n",
    "        self.sources = sources\n",
    "        self.base_models = base_models\n",
    "        self.day = day\n",
    "        self.fold = fold\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.inter_set_train = inter_set_train\n",
    "        self.inter_set_test = inter_set_test\n",
    "    \n",
    "    def build_data_loaders(self) -> None:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = load_train_test(\n",
    "            self.sources,\n",
    "            self.base_models,\n",
    "            self.day,\n",
    "            self.fold,\n",
    "            self.label_to_idx,\n",
    "            self.inter_set_train,\n",
    "            self.inter_set_test\n",
    "        )\n",
    "        \n",
    "        self.y_test = y_test\n",
    "        self.dim = X_train.shape[1]\n",
    "        self.n_labels = len(self.label_to_idx)\n",
    "\n",
    "        train_set = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "        test_set = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "        # Split the dataset into training and validation sets\n",
    "        train_len = int(0.9 * len(train_set))\n",
    "        val_len = len(train_set) - train_len\n",
    "\n",
    "        train_set, val_set = random_split(train_set, [train_len, val_len])\n",
    "\n",
    "        # Create DataLoaders\n",
    "        self.train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4)\n",
    "        self.val_loader = DataLoader(val_set, batch_size=64, num_workers=4)\n",
    "        self.test_loader = DataLoader(test_set, batch_size=64, num_workers=4)\n",
    "\n",
    "class NeuralNet(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=1e-3):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Define layers\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.criterion(out, y)\n",
    "        self.log_dict({\"val_loss\": loss }, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return preds\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Linear learning rate scheduler\n",
    "        scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 1 - epoch / self.trainer.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "def get_intersec(strat: dict, day: str, sources: list, fold: int) -> tuple:\n",
    "    \n",
    "    train_ips = set(strat[day][sources[0]][fold][0]).intersection(strat[day][sources[1]][fold][0])\n",
    "    test_ips = set(strat[day][sources[0]][fold][1]).intersection(strat[day][sources[1]][fold][1])\n",
    "    return train_ips, test_ips\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES = [\"darknet\", \"honeypot\"]\n",
    "BASE_MODELS = [\"features\", \"idarkvec\", \"igcngru_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 2\n",
    "MAX_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_cols = [\n",
    "    \"censys\",\n",
    "    \"driftnet\",\n",
    "    \"internetcensus\",\n",
    "    \"intrinsec\",\n",
    "    \"ipip\",\n",
    "    \"mirai\",\n",
    "    \"onyphe\",\n",
    "    \"rapid7\",\n",
    "    \"securitytrails\",\n",
    "    \"shadowserver\",\n",
    "    \"shodan\",\n",
    "    \"u_mich\",\n",
    "    \"unk_bruteforcer\",\n",
    "    \"unk_exploiter\",\n",
    "    \"unk_spammer\",\n",
    "    \"unknown\"\n",
    "]\n",
    "probs_cols.sort()\n",
    "\n",
    "label_to_idx = {l: idx for idx, l in enumerate(probs_cols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = sorted([ f.split('/')[-1].split('_')[-2] for f in glob(f\"data/2022/input/reps/out/k3/darknet/test/idarkvec*_fold00.csv\") ])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/2022/input/skf/stratification/stratification.json\", 'r') as fd:\n",
    "    strat = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for day, fold in product(days, np.arange(N_FOLDS)):\n",
    "    \n",
    "    # Getting intersection sets.\n",
    "    train_ips, test_ips = get_intersec(strat, day, SOURCES, fold)\n",
    "    \n",
    "    # Loading data.\n",
    "    data = DataHandler(SOURCES, BASE_MODELS, day, fold, label_to_idx, train_ips, test_ips)\n",
    "    data.build_data_loaders()\n",
    "    \n",
    "    # Setting network's size\n",
    "    input_dim = data.dim\n",
    "    hidden_dim = input_dim\n",
    "    output_dim = data.n_labels\n",
    "    \n",
    "    # Model Logger.\n",
    "    output = f\"data/2022/output/nn/1/{data.day}/{data.fold}/\"\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "\n",
    "    checkpoint_callbacker = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=output,\n",
    "        filename=\"model\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "    # Define model, trainer, and train the model\n",
    "    model = NeuralNet(\n",
    "        input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, lr=1e-3\n",
    "    )\n",
    "    trainer = pl.Trainer(max_epochs=MAX_EPOCHS, callbacks=[checkpoint_callbacker])\n",
    "    trainer.fit(model, data.train_loader, data.val_loader)\n",
    "    \n",
    "    # Load the best checkpoint\n",
    "    best_model_path = checkpoint_callbacker.best_model_path\n",
    "    best_model = NeuralNet.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    # Prediction\n",
    "    y_hat = trainer.predict(best_model, dataloaders=data.test_loader)\n",
    "    y_hat = (\n",
    "        torch.cat(y_hat).cpu().numpy()\n",
    "    )  # Combine y_hat and move to CPU\n",
    "\n",
    "    print(F\"DAY: {day} / FOLD: {fold} - M-F1: {(100 * f1_score(data.y_test, y_hat, average='macro')):.2f}\")\n",
    "    if day not in predictions:\n",
    "        predictions[day] = {}\n",
    "    predictions[day][fold] = {\n",
    "        \"y_test\": data.y_test,\n",
    "        \"y_hat\": y_hat\n",
    "    }\n",
    "    os.system(\"clear\")\n",
    "\n",
    "output = f\"data/2022/output/nn/1\"\n",
    "with open(f\"{output}/preds.pkl\", \"wb\") as fd:\n",
    "    pickle.dump(predictions, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
